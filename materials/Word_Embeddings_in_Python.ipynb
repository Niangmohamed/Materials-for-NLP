{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Word_Embeddings_in_Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS-2_mekMbNY"
      },
      "source": [
        "<h1 align=center><font size=5>Word Embeddings in Python</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHNxTavtMbNb"
      },
      "source": [
        "### Table of contents\n",
        "\n",
        "- [Objective](#objective)\n",
        "- [One-hot encoding](#one_hot)\n",
        "- [Encode each word with a unique number](#integer_enc)\n",
        "- [Word embeddings](#word_embeddings)\n",
        "- [References](#ref)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrE7FKFMbNb"
      },
      "source": [
        "### Objective <a id=\"objective\"></a>\n",
        "\n",
        "In this notebook, we learn different ways for converting strings to numbers (or to vectorize the text) before feeding it to machine learning models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZyQvbiGMbNc"
      },
      "source": [
        "### One-hot encoding <a id=\"one_hot\"></a>\n",
        "\n",
        "As a first idea, we might \"one-hot\" encode each word in our vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, we will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOCFT0K1MbNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e4e9a8-991e-45d1-f5bf-3a4e5bc2ca1a"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = 'The   cat sat on  the mat.'\n",
        "text = text.lower().split()\n",
        "print(text)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(text)\n",
        "print(integer_encoded)\n",
        "\n",
        "onehot_encoded = to_categorical(integer_encoded)\n",
        "print(onehot_encoded)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cat', 'sat', 'on', 'the', 'mat.']\n",
            "[4 0 3 2 4 1]\n",
            "[[0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nh7fxwWMbNh"
      },
      "source": [
        "&#x270d; What are downsides to this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TQL81YNMbNj"
      },
      "source": [
        "> This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indicices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxDQ4L2QMbNk"
      },
      "source": [
        "### Encode each word with a unique number <a id=\"integer_enc\"></a>\n",
        "\n",
        "A second approach we might try is to encode each word using a unique number. Continuing the example above, we could assign 1 to \"cat\", 2 to \"mat\", and so on. We could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. \n",
        "\n",
        "&#x270d; What are pros and cons of this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWvdlQexMbNl"
      },
      "source": [
        "> This appoach is efficient. Instead of a sparse vector, we now have a dense one (where all elements are full).\n",
        "\n",
        "> There are two downsides to this approach, however:\n",
        "    - The integer-encoding is arbitrary (it does not capture any relationship between words).\n",
        "    - An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx22BZx3MbNm"
      },
      "source": [
        "Text tokenization utility class in Tensorflow allows us to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "\n",
        "Arguments:\n",
        "\n",
        "- __num_words__: the maximum number of words to keep, based on word frequency. Only the most common `num_words-1` words will be kept.\n",
        "- __filters__: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the `'` character.\n",
        "- __lower__: boolean. Whether to convert the texts to lowercase.\n",
        "- __split__: str. Separator for word splitting.\n",
        "- __char_level__: if True, every character will be treated as a token.\n",
        "- __oov_token__: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
        "\n",
        "By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the `'` character). These sequences are then split into lists of tokens. They will then be indexed or vectorized. Note that `0` is a reserved index that won't be assigned to any word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQzhTGdgMbNm"
      },
      "source": [
        "#### Text Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6ncT92sMbNn"
      },
      "source": [
        "Here, we learn how to tokenize a text, and then turn sentences into sequences using tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0afxMLiMbNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329fea1b-6416-4e90-9eb8-9e3a5110bb28"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "texts = ['The cat sat on the mat.',\n",
        "         'The dog sat on the log.',\n",
        "         'Dogs and cats living together.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20) \n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts) # Transforms each text into a sequence of integers\n",
        "print('Sequences:\\n', sequences)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index:\n",
            " {'the': 1, 'sat': 2, 'on': 3, 'cat': 4, 'mat': 5, 'dog': 6, 'log': 7, 'dogs': 8, 'and': 9, 'cats': 10, 'living': 11, 'together': 12}\n",
            "Sequences:\n",
            " [[1, 4, 2, 3, 1, 5], [1, 6, 2, 3, 1, 7], [8, 9, 10, 11, 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_yY5GQfMbNr"
      },
      "source": [
        "#### Test Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKwaxYghMbNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701b4428-b938-4e25-fb0e-0ea0034154eb"
      },
      "source": [
        "X_train = ['The cat sat on the mat.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20) \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "print('Sequences:\\n', X_train_seq)\n",
        "# --------------------------------------------------------\n",
        "X_test = ['The dog sat on the log.']\n",
        "\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "print('Test sequence:\\n', X_test_seq)# here the unseen words has ignored"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index:\n",
            " {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
            "Sequences:\n",
            " [[1, 2, 3, 4, 1, 5]]\n",
            "Test sequence:\n",
            " [[1, 3, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVyi8kXBMbNu"
      },
      "source": [
        "#### Out Of Vocabulary (OOV) words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKEnpSsPMbNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44969841-8bf2-4610-bc4c-0e28839f99a7"
      },
      "source": [
        "X_train = ['The cat sat on the mat.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token = '<OOV>') \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "print('Sequences:\\n', X_train_seq)\n",
        "# --------------------------------------------------------\n",
        "X_test = ['The dog sat on the log.']\n",
        "\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "print('Test sequence:\\n', X_test_seq)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index:\n",
            " {'<OOV>': 1, 'the': 2, 'cat': 3, 'sat': 4, 'on': 5, 'mat': 6}\n",
            "Sequences:\n",
            " [[2, 3, 4, 5, 2, 6]]\n",
            "Test sequence:\n",
            " [[2, 1, 4, 5, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enveOUA6MbNx"
      },
      "source": [
        "#### Padding <br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09M2Q9IMbNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9a720b-c8a3-43c9-c56b-051e54b00288"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['I love my dog', \n",
        "             'You love my dog!',\n",
        "             'Do you think my dog is amazing?']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token = '<OOV>') \n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:\\n', word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print('Sequences:\\n', sequences)\n",
        "\n",
        "padded = pad_sequences(sequences)\n",
        "print('Padded sequences:\\n', padded)\n",
        "\n",
        "matrix2 = tokenizer.texts_to_matrix(['I love my dog']) \n",
        "print(matrix2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index:\n",
            " {'<OOV>': 1, 'my': 2, 'dog': 3, 'love': 4, 'you': 5, 'i': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "Sequences:\n",
            " [[6, 4, 2, 3], [5, 4, 2, 3], [7, 5, 8, 2, 3, 9, 10]]\n",
            "Padded sequences:\n",
            " [[ 0  0  0  6  4  2  3]\n",
            " [ 0  0  0  5  4  2  3]\n",
            " [ 7  5  8  2  3  9 10]]\n",
            "[[0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5pBNdNJMbN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e683b984-40a0-41f3-e130-66a99d77bf10"
      },
      "source": [
        "padded = pad_sequences(sequences, padding = 'post', maxlen = 5, truncating = 'post') \n",
        "print('Padded sequences:\\n', padded)\n",
        "print('Padded shape:', padded.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded sequences:\n",
            " [[6 4 2 3 0]\n",
            " [5 4 2 3 0]\n",
            " [7 5 8 2 3]]\n",
            "Padded shape: (3, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4AM9sDcMbN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35840307-23b6-44ab-9d97-a8daf9fd814a"
      },
      "source": [
        "texts = ['The the the the the cat sat on the mat cat.']\n",
        "tokenizer = Tokenizer(num_words = 10) \n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Word index:', word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print('Sequences:', sequences)\n",
        "\n",
        "for mode in ['binary', 'count', 'freq', 'tfidf']:\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode) # Convert a list of texts to a Numpy matrix.\n",
        "    print('-'*20, mode, '-'*20)\n",
        "    print(matrix)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index: {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
            "Sequences: [[1, 1, 1, 1, 1, 2, 3, 4, 1, 5, 2]]\n",
            "-------------------- binary --------------------\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]]\n",
            "-------------------- count --------------------\n",
            "[[0. 6. 2. 1. 1. 1. 0. 0. 0. 0.]]\n",
            "-------------------- freq --------------------\n",
            "[[0.         0.54545455 0.18181818 0.09090909 0.09090909 0.09090909\n",
            "  0.         0.         0.         0.        ]]\n",
            "-------------------- tfidf --------------------\n",
            "[[0.         1.13196106 0.6865121  0.40546511 0.40546511 0.40546511\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5IvwQ5NMbN4"
      },
      "source": [
        "### Word embeddings <a id=\"word_embeddings\"></a>\n",
        "\n",
        "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, we do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2AxD2zMbN5"
      },
      "source": [
        "### References <a id=\"ref\"></a>\n",
        "\n",
        "- https://keras.io/preprocessing/text/\n",
        "    \n",
        "- https://www.tensorflow.org/tutorials/text/word_embeddings"
      ]
    }
  ]
}